{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEmzn6AbBJ7tp1BaH8q4CC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ocehuem/hw6/blob/main/Untitled28.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decision tree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def entropy(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "def information_gain(X, y, feature_index, threshold):\n",
        "    parent_entropy = entropy(y)\n",
        "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
        "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "    if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "        return 0\n",
        "\n",
        "    left_entropy = entropy(y[left_indices])\n",
        "    right_entropy = entropy(y[right_indices])\n",
        "\n",
        "    num_left, num_right = len(left_indices), len(right_indices)\n",
        "    num_total = num_left + num_right\n",
        "\n",
        "    return parent_entropy - ((num_left / num_total) * left_entropy + (num_right / num_total) * right_entropy)\n",
        "\n",
        "def find_best_split(X, y):\n",
        "    best_feature_index, best_threshold, max_gain = None, None, -np.inf\n",
        "    num_samples, num_features = X.shape\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        feature_values = np.unique(X[:, feature_index])\n",
        "        for threshold in feature_values:\n",
        "            gain = information_gain(X, y, feature_index, threshold)\n",
        "            if gain > max_gain:\n",
        "                best_feature_index, best_threshold, max_gain = feature_index, threshold, gain\n",
        "\n",
        "    return best_feature_index, best_threshold\n",
        "\n",
        "def grow_tree(X, y, max_depth=None, min_samples_split=2, depth=0):\n",
        "    num_samples, num_features = X.shape\n",
        "    num_labels = len(np.unique(y))\n",
        "\n",
        "    if num_labels == 1 or num_samples < min_samples_split or (max_depth is not None and depth >= max_depth):\n",
        "        return np.argmax(np.bincount(y))\n",
        "\n",
        "    best_feature_index, best_threshold = find_best_split(X, y)\n",
        "\n",
        "    if best_feature_index is None:\n",
        "        return np.argmax(np.bincount(y))\n",
        "\n",
        "    left_indices = np.where(X[:, best_feature_index] <= best_threshold)[0]\n",
        "    right_indices = np.where(X[:, best_feature_index] > best_threshold)[0]\n",
        "\n",
        "    left_node = grow_tree(X[left_indices], y[left_indices], max_depth, min_samples_split, depth + 1)\n",
        "    right_node = grow_tree(X[right_indices], y[right_indices], max_depth, min_samples_split, depth + 1)\n",
        "\n",
        "    return {'feature_index': best_feature_index, 'threshold': best_threshold, 'left': left_node, 'right': right_node}\n",
        "\n",
        "def predict_one_sample(x, tree):\n",
        "    if isinstance(tree, int):\n",
        "        return tree\n",
        "\n",
        "    if x[tree['feature_index']] <= tree['threshold']:\n",
        "        return predict_one_sample(x, tree['left'])\n",
        "    else:\n",
        "        return predict_one_sample(x, tree['right'])\n",
        "\n",
        "def predict(X, tree):\n",
        "    return np.array([predict_one_sample(x, tree) for x in X])\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Train decision tree\n",
        "    tree = grow_tree(X, y, max_depth=3)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict(X, tree)\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "id": "3sB_w-fxxsDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kmeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def assign_clusters(X, centroids):\n",
        "    clusters = []\n",
        "    for point in X:\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        cluster = np.argmin(distances)\n",
        "        clusters.append(cluster)\n",
        "    return np.array(clusters)\n",
        "\n",
        "def update_centroids(X, clusters, num_clusters):\n",
        "    centroids = []\n",
        "    for i in range(num_clusters):\n",
        "        cluster_points = X[clusters == i]\n",
        "        centroid = np.mean(cluster_points, axis=0)\n",
        "        centroids.append(centroid)\n",
        "    return np.array(centroids)\n",
        "\n",
        "def kmeans(X, num_clusters, max_iterations=100):\n",
        "    # Randomly initialize centroids\n",
        "    centroids = X[np.random.choice(X.shape[0], num_clusters, replace=False)]\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        # Assign points to nearest centroid\n",
        "        clusters = assign_clusters(X, centroids)\n",
        "        # Update centroids based on cluster means\n",
        "        new_centroids = update_centroids(X, clusters, num_clusters)\n",
        "        # Check for convergence\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return clusters, centroids\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.values\n",
        "    return X\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X = load_data(file_path)\n",
        "\n",
        "    # Set number of clusters\n",
        "    num_clusters = 3\n",
        "\n",
        "    # Perform K-Means clustering\n",
        "    clusters, centroids = kmeans(X, num_clusters)\n",
        "\n",
        "    # Print cluster assignments and centroids\n",
        "    print(\"Cluster Assignments:\", clusters)\n",
        "    print(\"Centroids:\", centroids)\n"
      ],
      "metadata": {
        "id": "nBZkFIRryIQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#knn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def knn(X_train, y_train, X_test, k=5):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        distances = [euclidean_distance(test_point, train_point) for train_point in X_train]\n",
        "        nearest_indices = np.argsort(distances)[:k]\n",
        "        nearest_labels = y_train[nearest_indices]\n",
        "        unique_labels, label_counts = np.unique(nearest_labels, return_counts=True)\n",
        "        predicted_label = unique_labels[np.argmax(label_counts)]\n",
        "        predictions.append(predicted_label)\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Split data into train and test sets (80% train, 20% test)\n",
        "    num_train_samples = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:num_train_samples], X[num_train_samples:]\n",
        "    y_train, y_test = y[:num_train_samples], y[num_train_samples:]\n",
        "\n",
        "    # Perform KNN classification\n",
        "    predictions = knn(X_train, y_train, X_test, k=5)\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "id": "R1hInRGFyV6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j58_I6g4a6xh"
      },
      "outputs": [],
      "source": [
        "#neural networks deep learning stochastic gradient descent\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    return a2, z1, a1, z2\n",
        "\n",
        "def backward(X, y, a2, z1, a1, z2, W1, W2, b1, b2, learning_rate):\n",
        "    m = len(X)\n",
        "    delta2 = (a2 - y) * sigmoid_derivative(a2)\n",
        "    dW2 = np.dot(a1.T, delta2) / m\n",
        "    db2 = np.sum(delta2, axis=0) / m\n",
        "    delta1 = np.dot(delta2, W2.T) * sigmoid_derivative(a1)\n",
        "    dW1 = np.dot(X.T, delta1) / m\n",
        "    db1 = np.sum(delta1, axis=0) / m\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def fit(X, y, epochs=1000, learning_rate=0.01):\n",
        "    input_size = X.shape[1]\n",
        "    hidden_size = 4\n",
        "    output_size = 1\n",
        "    W1 = np.random.randn(input_size, hidden_size)\n",
        "    b1 = np.random.randn(hidden_size)\n",
        "    W2 = np.random.randn(hidden_size, output_size)\n",
        "    b2 = np.random.randn(output_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        a2, z1, a1, z2 = forward(X, W1, b1, W2, b2)\n",
        "        W1, b1, W2, b2 = backward(X, y, a2, z1, a1, z2, W1, W2, b1, b2, learning_rate)\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    a2, _, _, _ = forward(X, W1, b1, W2, b2)\n",
        "    return a2\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values.reshape(-1, 1)\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    X, y = load_data(\"data.csv\")\n",
        "\n",
        "    # Train neural network using SGD\n",
        "    W1, b1, W2, b2 = fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict(X, W1, b1, W2, b2)\n",
        "    print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deep learning neuralnetworks,multilayered-fully connected\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def forward(X, weights, biases):\n",
        "    activations = [X]\n",
        "    pre_activations = []\n",
        "    for i in range(len(weights)):\n",
        "        pre_activation = np.dot(activations[-1], weights[i]) + biases[i]\n",
        "        pre_activations.append(pre_activation)\n",
        "        activations.append(sigmoid(pre_activation))\n",
        "    return activations, pre_activations\n",
        "\n",
        "def backward(X, y, activations, pre_activations, weights, biases, learning_rate):\n",
        "    m = len(X)\n",
        "    gradients_weights = [None] * len(weights)\n",
        "    gradients_biases = [None] * len(biases)\n",
        "\n",
        "    delta = (activations[-1] - y) * sigmoid_derivative(activations[-1])\n",
        "    for i in range(len(weights) - 1, -1, -1):\n",
        "        gradients_weights[i] = np.dot(activations[i].T, delta) / m\n",
        "        gradients_biases[i] = np.sum(delta, axis=0) / m\n",
        "        if i > 0:\n",
        "            delta = np.dot(delta, weights[i].T) * sigmoid_derivative(activations[i])\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate * gradients_weights[i]\n",
        "        biases[i] -= learning_rate * gradients_biases[i]\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "def fit(X, y, layer_sizes, epochs=1000, learning_rate=0.01):\n",
        "    input_size = X.shape[1]\n",
        "    output_size = y.shape[1]\n",
        "    weights = [np.random.randn(input_size, layer_sizes[0])]\n",
        "    biases = [np.random.randn(layer_sizes[0])]\n",
        "    for i in range(1, len(layer_sizes)):\n",
        "        weights.append(np.random.randn(layer_sizes[i-1], layer_sizes[i]))\n",
        "        biases.append(np.random.randn(layer_sizes[i]))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        activations, pre_activations = forward(X, weights, biases)\n",
        "        weights, biases = backward(X, y, activations, pre_activations, weights, biases, learning_rate)\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "def predict(X, weights, biases):\n",
        "    activations, _ = forward(X, weights, biases)\n",
        "    return activations[-1]\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values.reshape(-1, 1)  # Adjust if there are multiple output neurons\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    X, y = load_data(\"data.csv\")\n",
        "\n",
        "    # Define network architecture\n",
        "    layer_sizes = [4, 3]  # Adjust according to the number of neurons in each hidden layer\n",
        "\n",
        "    # Train neural network using SGD\n",
        "    weights, biases = fit(X, y, layer_sizes)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict(X, weights, biases)\n",
        "    print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "id": "EzBRFCNIr8UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pca\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def pca(X, num_components):\n",
        "    # Normalize the data\n",
        "    X_mean = np.mean(X, axis=0)\n",
        "    X_normalized = X - X_mean\n",
        "\n",
        "    # Calculate the covariance matrix\n",
        "    covariance_matrix = np.cov(X_normalized, rowvar=False)\n",
        "\n",
        "    # Calculate eigenvectors and eigenvalues of the covariance matrix\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
        "\n",
        "    # Sort eigenvalues and eigenvectors in descending order\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    sorted_eigenvalues = eigenvalues[sorted_indices]\n",
        "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Select the top num_components eigenvectors\n",
        "    principal_components = sorted_eigenvectors[:, :num_components]\n",
        "\n",
        "    # Project the data onto the selected principal components\n",
        "    projected_data = np.dot(X_normalized, principal_components)\n",
        "\n",
        "    return projected_data\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.values\n",
        "    return X\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X = load_data(file_path)\n",
        "\n",
        "    # Number of principal components\n",
        "    num_components = 2\n",
        "\n",
        "    # Apply PCA\n",
        "    projected_data = pca(X, num_components)\n",
        "\n",
        "    # Print the projected data\n",
        "    print(\"Projected Data:\")\n",
        "    print(projected_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAkBsYg2sRKP",
        "outputId": "6429f6e9-5d47-492a-9cfa-e9d9f57ed6db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projected Data:\n",
            "[[ 6.05656646e+00 -7.41800315e-02 -1.94289029e-16]\n",
            " [ 9.12046501e-01  1.16433293e+00  2.77555756e-17]\n",
            " [-4.23247346e+00  2.40284590e+00  2.49800181e-16]\n",
            " [-2.73613950e+00 -3.49299880e+00 -8.32667268e-17]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#kmeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def initialize_centroids(X, k):\n",
        "    # Randomly select k data points as initial centroids\n",
        "    indices = np.random.choice(X.shape[0], k, replace=False)\n",
        "    centroids = X[indices]\n",
        "    return centroids\n",
        "\n",
        "def assign_clusters(X, centroids):\n",
        "    # Compute distances between each data point and centroids\n",
        "    distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n",
        "    # Assign each data point to the closest centroid\n",
        "    clusters = np.argmin(distances, axis=1)\n",
        "    return clusters\n",
        "\n",
        "def update_centroids(X, clusters, k):\n",
        "    centroids = np.zeros((k, X.shape[1]))\n",
        "    for i in range(k):\n",
        "        # Compute the mean of data points assigned to each centroid\n",
        "        centroids[i] = np.mean(X[clusters == i], axis=0)\n",
        "    return centroids\n",
        "\n",
        "def kmeans(X, k, max_iterations=100):\n",
        "    # Initialize centroids\n",
        "    centroids = initialize_centroids(X, k)\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        # Assign data points to clusters\n",
        "        clusters = assign_clusters(X, centroids)\n",
        "        # Update centroids\n",
        "        new_centroids = update_centroids(X, clusters, k)\n",
        "        # Check for convergence\n",
        "        if np.allclose(new_centroids, centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return clusters, centroids\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.values\n",
        "    return X\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X = load_data(file_path)\n",
        "\n",
        "    # Number of clusters\n",
        "    k = 3\n",
        "\n",
        "    # Run K-Means algorithm\n",
        "    clusters, centroids = kmeans(X, k)\n",
        "\n",
        "    # Print the cluster assignments and centroids\n",
        "    print(\"Cluster assignments:\", clusters)\n",
        "    print(\"Centroids:\")\n",
        "    print(centroids)\n"
      ],
      "metadata": {
        "id": "OJmg47RltM5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, learning_rate=0.01, num_iterations=1000):\n",
        "    m, n = X.shape\n",
        "    # Initialize weights\n",
        "    weights = np.zeros((n, 1))\n",
        "    # Add bias term to X\n",
        "    X = np.hstack((np.ones((m, 1)), X))\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # Compute predictions\n",
        "        predictions = sigmoid(np.dot(X, weights))\n",
        "        # Compute gradients\n",
        "        gradient = np.dot(X.T, (predictions - y)) / m\n",
        "        # Update weights\n",
        "        weights -= learning_rate * gradient\n",
        "\n",
        "    return weights\n",
        "\n",
        "def predict(X, weights):\n",
        "    # Add bias term to X\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    # Compute predictions\n",
        "    predictions = sigmoid(np.dot(X, weights))\n",
        "    # Convert probabilities to binary predictions\n",
        "    binary_predictions = (predictions >= 0.5).astype(int)\n",
        "    return binary_predictions\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values.reshape(-1, 1)\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Train logistic regression model\n",
        "    weights = logistic_regression(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict(X, weights)\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Predictions:\")\n",
        "    print(predictions)\n"
      ],
      "metadata": {
        "id": "Xqma397EtebD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear regression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def linear_regression(X, y):\n",
        "    # Add bias term to X\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    # Compute weights using closed-form solution (normal equation)\n",
        "    weights = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
        "    return weights\n",
        "\n",
        "def predict(X, weights):\n",
        "    # Add bias term to X\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    # Compute predictions\n",
        "    predictions = np.dot(X, weights)\n",
        "    return predictions\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values.reshape(-1, 1)\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Train linear regression model\n",
        "    weights = linear_regression(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict(X, weights)\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Predictions:\")\n",
        "    print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "YZsbbS0rtx_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def lms(X, y, learning_rate=0.01, num_iterations=1000):\n",
        "    m, n = X.shape\n",
        "    # Add bias term to X\n",
        "    X = np.hstack((np.ones((m, 1)), X))\n",
        "    # Initialize weights\n",
        "    weights = np.zeros((n + 1, 1))\n",
        "    for _ in range(num_iterations):\n",
        "        for i in range(m):\n",
        "            # Compute prediction\n",
        "            prediction = np.dot(X[i], weights)\n",
        "            # Compute error\n",
        "            error = y[i] - prediction\n",
        "            # Update weights\n",
        "            weights += learning_rate * error * X[i].reshape(-1, 1)\n",
        "    return weights\n",
        "\n",
        "def weighted_lms(X, y, weights, learning_rate=0.01, num_iterations=1000):\n",
        "    m, n = X.shape\n",
        "    # Add bias term to X\n",
        "    X = np.hstack((np.ones((m, 1)), X))\n",
        "    for _ in range(num_iterations):\n",
        "        for i in range(m):\n",
        "            # Compute prediction\n",
        "            prediction = np.dot(X[i], weights)\n",
        "            # Compute error\n",
        "            error = y[i] - prediction\n",
        "            # Update weights with weights\n",
        "            weights += learning_rate * error * X[i].reshape(-1, 1) * weights[i]\n",
        "    return weights\n",
        "\n",
        "def predict(X, weights):\n",
        "    # Add bias term to X\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    # Compute predictions\n",
        "    predictions = np.dot(X, weights)\n",
        "    return predictions\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values.reshape(-1, 1)\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Train linear regression model using LMS\n",
        "    weights_lms = lms(X, y)\n",
        "\n",
        "    # Train linear regression model using weighted LMS\n",
        "    weights_weighted_lms = weighted_lms(X, y, np.ones((X.shape[0], 1)))\n",
        "\n",
        "    # Make predictions\n",
        "    predictions_lms = predict(X, weights_lms)\n",
        "    predictions_weighted_lms = predict(X, weights_weighted_lms)\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Predictions (LMS):\")\n",
        "    print(predictions_lms)\n",
        "    print(\"Predictions (Weighted LMS):\")\n",
        "    print(predictions_weighted_lms)\n"
      ],
      "metadata": {
        "id": "8oS0os2buTO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear regression locally_weighted_regression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def locally_weighted_regression(X_train, y_train, x, tau=0.1):\n",
        "    m, n = X_train.shape\n",
        "    # Add bias term to X_train\n",
        "    X_train = np.hstack((np.ones((m, 1)), X_train))\n",
        "    # Add bias term to x\n",
        "    x = np.hstack(([1], x))\n",
        "    # Initialize weights matrix\n",
        "    weights = np.eye(m)\n",
        "    # Compute weights for each data point\n",
        "    for i in range(m):\n",
        "        xi = X_train[i]\n",
        "        weights[i, i] = np.exp(-np.sum((xi - x)**2) / (2 * tau**2))\n",
        "    # Compute theta using weighted least squares\n",
        "    theta = np.linalg.inv(X_train.T.dot(weights).dot(X_train)).dot(X_train.T).dot(weights).dot(y_train)\n",
        "    # Compute prediction using learned theta\n",
        "    prediction = x.dot(theta)\n",
        "    return prediction, theta\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X_train = df.drop(columns=['target']).values\n",
        "    y_train = df['target'].values\n",
        "    return X_train, y_train\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load training data\n",
        "    file_path = \"data.csv\"\n",
        "    X_train, y_train = load_data(file_path)\n",
        "\n",
        "    # Define test point\n",
        "    x_test = np.array([0.5, 0.5])  # Adjust according to your test point\n",
        "\n",
        "    # Set bandwidth parameter (tau)\n",
        "    tau = 0.1  # Adjust according to your preference\n",
        "\n",
        "    # Perform locally weighted regression\n",
        "    prediction, theta = locally_weighted_regression(X_train, y_train, x_test, tau)\n",
        "\n",
        "    # Print prediction and theta\n",
        "    print(\"Prediction for test point:\", prediction)\n",
        "    print(\"Learned theta:\", theta)\n"
      ],
      "metadata": {
        "id": "x0vJsKb-uaqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear regression lagrangian_optimizatio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def loss_function(weights, X, y):\n",
        "    predictions = np.dot(X, weights)\n",
        "    squared_error = np.sum((predictions - y) ** 2)\n",
        "    return squared_error\n",
        "\n",
        "def constraint_function(weights, constraint_value):\n",
        "    return np.sum(np.abs(weights)) - constraint_value\n",
        "\n",
        "def lagrangian_optimization(X, y, constraint_value, learning_rate=0.01, num_iterations=1000):\n",
        "    m, n = X.shape\n",
        "    # Initialize weights\n",
        "    weights = np.zeros(n)\n",
        "    for _ in range(num_iterations):\n",
        "        # Compute gradient of loss function\n",
        "        gradient = np.dot(X.T, np.dot(X, weights) - y)\n",
        "        # Update weights\n",
        "        weights -= learning_rate * gradient\n",
        "        # Project weights onto feasible region\n",
        "        weights = project_weights(weights, constraint_value)\n",
        "    return weights\n",
        "\n",
        "def project_weights(weights, constraint_value):\n",
        "    weights_abs = np.abs(weights)\n",
        "    if np.sum(weights_abs) <= constraint_value:\n",
        "        return weights\n",
        "    else:\n",
        "        weights_sorted = np.sort(weights_abs)[::-1]\n",
        "        cumulative_sum = np.cumsum(weights_sorted)\n",
        "        rho = np.nonzero(weights_sorted * np.arange(1, len(weights_sorted) + 1) > (cumulative_sum - constraint_value))[0][-1]\n",
        "        theta = (cumulative_sum[rho] - constraint_value) / (rho + 1)\n",
        "        return np.sign(weights) * np.maximum(weights_abs - theta, 0)\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Set constraint value\n",
        "    constraint_value = 10  # Adjust as needed\n",
        "\n",
        "    # Perform optimization using Lagrangian\n",
        "    optimal_weights = lagrangian_optimization(X, y, constraint_value)\n",
        "\n",
        "    # Print optimal weights\n",
        "    print(\"Optimal weights:\", optimal_weights)\n"
      ],
      "metadata": {
        "id": "mCJhYHZIwJN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def loss_function(weights, X, y):\n",
        "    predictions = np.dot(X, weights)\n",
        "    squared_error = np.sum((predictions - y) ** 2)\n",
        "    return squared_error\n",
        "\n",
        "def gradient(weights, X, y):\n",
        "    return np.dot(X.T, np.dot(X, weights) - y)\n",
        "\n",
        "def hessian(X):\n",
        "    return np.dot(X.T, X)\n",
        "\n",
        "def constraint_function(weights, constraint_value):\n",
        "    return np.sum(np.abs(weights)) - constraint_value\n",
        "\n",
        "def lagrangian_optimization(X, y, constraint_value, learning_rate=0.1, num_iterations=100):\n",
        "    m, n = X.shape\n",
        "    # Initialize weights\n",
        "    weights = np.zeros(n)\n",
        "    for _ in range(num_iterations):\n",
        "        grad = gradient(weights, X, y)\n",
        "        hess = hessian(X)\n",
        "        # Compute Lagrange multiplier\n",
        "        lambda_ = np.linalg.solve(hess, grad)\n",
        "        # Update weights using Lagrange multiplier\n",
        "        weights -= learning_rate * lambda_\n",
        "        # Project weights onto feasible region\n",
        "        weights = project_weights(weights, constraint_value)\n",
        "    return weights\n",
        "\n",
        "def project_weights(weights, constraint_value):\n",
        "    weights_abs = np.abs(weights)\n",
        "    if np.sum(weights_abs) <= constraint_value:\n",
        "        return weights\n",
        "    else:\n",
        "        weights_sorted = np.sort(weights_abs)[::-1]\n",
        "        cumulative_sum = np.cumsum(weights_sorted)\n",
        "        rho = np.nonzero(weights_sorted * np.arange(1, len(weights_sorted) + 1) > (cumulative_sum - constraint_value))[0][-1]\n",
        "        theta = (cumulative_sum[rho] - constraint_value) / (rho + 1)\n",
        "        return np.sign(weights) * np.maximum(weights_abs - theta, 0)\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values\n",
        "    return X, y\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Set constraint value\n",
        "    constraint_value = 10  # Adjust as needed\n",
        "\n",
        "    # Perform optimization using Lagrangian with Newton's method\n",
        "    optimal_weights = lagrangian_optimization(X, y, constraint_value)\n",
        "\n",
        "    # Print optimal weights\n",
        "    print(\"Optimal weights:\", optimal_weights)\n"
      ],
      "metadata": {
        "id": "SLZNKGUPwPrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def linear_regression_sgd(X, y, learning_rate=0.01, num_iterations=1000, batch_size=1):\n",
        "    m, n = X.shape\n",
        "    # Initialize weights\n",
        "    weights = np.zeros(n)\n",
        "    for _ in range(num_iterations):\n",
        "        # Shuffle the data\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "        for i in range(0, m, batch_size):\n",
        "            # Take a batch of data\n",
        "            X_batch = X_shuffled[i:i+batch_size]\n",
        "            y_batch = y_shuffled[i:i+batch_size]\n",
        "            # Compute gradient\n",
        "            gradient = np.dot(X_batch.T, np.dot(X_batch, weights) - y_batch)\n",
        "            # Update weights\n",
        "            weights -= learning_rate * gradient / len(X_batch)\n",
        "    return weights\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values\n",
        "    return X, y\n",
        "\n",
        "# Example usage for linear regression:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Perform optimization using Stochastic Gradient Descent for Linear Regression\n",
        "    optimal_weights = linear_regression_sgd(X, y)\n",
        "\n",
        "    # Print optimal weights\n",
        "    print(\"Optimal weights:\", optimal_weights)\n"
      ],
      "metadata": {
        "id": "6kXecE3pwiao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def logistic_regression_sgd(X, y, learning_rate=0.01, num_iterations=1000, batch_size=1):\n",
        "    m, n = X.shape\n",
        "    # Initialize weights\n",
        "    weights = np.zeros(n)\n",
        "    for _ in range(num_iterations):\n",
        "        # Shuffle the data\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "        for i in range(0, m, batch_size):\n",
        "            # Take a batch of data\n",
        "            X_batch = X_shuffled[i:i+batch_size]\n",
        "            y_batch = y_shuffled[i:i+batch_size]\n",
        "            # Compute gradient\n",
        "            gradient = np.dot(X_batch.T, sigmoid(np.dot(X_batch, weights)) - y_batch)\n",
        "            # Update weights\n",
        "            weights -= learning_rate * gradient / len(X_batch)\n",
        "    return weights\n",
        "\n",
        "# Load data from CSV file\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['target']).values\n",
        "    y = df['target'].values\n",
        "    return X, y\n",
        "\n",
        "# Example usage for logistic regression:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    file_path = \"data.csv\"\n",
        "    X, y = load_data(file_path)\n",
        "\n",
        "    # Perform optimization using Stochastic Gradient Descent for Logistic Regression\n",
        "    optimal_weights = logistic_regression_sgd(X, y)\n",
        "\n",
        "    # Print optimal weights\n",
        "    print(\"Optimal weights:\", optimal_weights)\n"
      ],
      "metadata": {
        "id": "S4Cfz6Y5w_S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bfs\n",
        "from collections import deque\n",
        "\n",
        "def bfs(graph, start):\n",
        "    visited = set()\n",
        "    queue = deque([start])\n",
        "    while queue:\n",
        "        vertex = queue.popleft()\n",
        "        if vertex not in visited:\n",
        "            visited.add(vertex)\n",
        "            print(vertex, end=\" \")\n",
        "            queue.extend(graph[vertex] - visited)\n",
        "\n",
        "# Example graph representation (adjacency list)\n",
        "graph = {\n",
        "    'A': {'B', 'C'},\n",
        "    'B': {'A', 'D', 'E'},\n",
        "    'C': {'A', 'F'},\n",
        "    'D': {'B'},\n",
        "    'E': {'B', 'F'},\n",
        "    'F': {'C', 'E'}\n",
        "}\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"BFS traversal:\")\n",
        "    bfs(graph, 'A')  # Start BFS traversal from vertex 'A'\n"
      ],
      "metadata": {
        "id": "Bl0eZ2HVyt6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dfs(graph, start):\n",
        "    visited = set()\n",
        "    stack = [start]\n",
        "    while stack:\n",
        "        vertex = stack.pop()\n",
        "        if vertex not in visited:\n",
        "            visited.add(vertex)\n",
        "            print(vertex, end=\" \")\n",
        "            stack.extend(graph[vertex] - visited)\n",
        "\n",
        "# Example graph representation (adjacency list)\n",
        "graph = {\n",
        "    'A': {'B', 'C'},\n",
        "    'B': {'A', 'D', 'E'},\n",
        "    'C': {'A', 'F'},\n",
        "    'D': {'B'},\n",
        "    'E': {'B', 'F'},\n",
        "    'F': {'C', 'E'}\n",
        "}\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"DFS traversal:\")\n",
        "    dfs(graph, 'A')  # Start DFS traversal from vertex 'A'\n"
      ],
      "metadata": {
        "id": "0m9NdyXHyvzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ucs\n",
        "import heapq\n",
        "\n",
        "def ucs(graph, start, goal):\n",
        "    visited = set()\n",
        "    queue = [(0, start, [])]  # Priority queue (cost, node, path)\n",
        "    while queue:\n",
        "        cost, node, path = heapq.heappop(queue)\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            path = path + [node]\n",
        "            if node == goal:\n",
        "                return cost, path\n",
        "            for neighbor, neighbor_cost in graph[node].items():\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(queue, (cost + neighbor_cost, neighbor, path))\n",
        "    return float('inf'), None\n",
        "\n",
        "# Example graph representation (adjacency list with edge costs)\n",
        "graph = {\n",
        "    'A': {'B': 1, 'C': 4},\n",
        "    'B': {'A': 1, 'D': 2, 'E': 5},\n",
        "    'C': {'A': 4, 'F': 3},\n",
        "    'D': {'B': 2},\n",
        "    'E': {'B': 5, 'F': 1},\n",
        "    'F': {'C': 3, 'E': 1}\n",
        "}\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    start_node = 'A'\n",
        "    goal_node = 'F'\n",
        "    cost, path = ucs(graph, start_node, goal_node)\n",
        "    if path:\n",
        "        print(\"UCS path from\", start_node, \"to\", goal_node, \":\", path)\n",
        "        print(\n"
      ],
      "metadata": {
        "id": "XYkL7z7yy83a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "def heuristic(node, goal):\n",
        "    # Example heuristic function: Euclidean distance between node and goal\n",
        "    return ((node[0] - goal[0])**2 + (node[1] - goal[1])**2)**0.5\n",
        "\n",
        "def astar(graph, start, goal):\n",
        "    visited = set()\n",
        "    queue = [(0 + heuristic(start, goal), 0, start, [])]  # Priority queue (f-score, g-score, node, path)\n",
        "    while queue:\n",
        "        _, cost, node, path = heapq.heappop(queue)\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            path = path + [node]\n",
        "            if node == goal:\n",
        "                return cost, path\n",
        "            for neighbor, neighbor_cost in graph[node].items():\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(queue, (cost + neighbor_cost + heuristic(neighbor, goal), cost + neighbor_cost, neighbor, path))\n",
        "    return float('inf'), None\n",
        "\n",
        "# Example graph representation (adjacency list with edge costs)\n",
        "graph = {\n",
        "    (0, 0): {(0, 1): 1, (1, 0): 1},\n",
        "    (0, 1): {(0, 0): 1, (1, 1): 1},\n",
        "    (1, 0): {(0, 0): 1, (1, 1): 1},\n",
        "    (1, 1): {(0, 1): 1, (1, 0): 1}\n",
        "}\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    start_node = (0, 0)\n",
        "    goal_node = (1, 1)\n",
        "    cost, path = astar(graph, start_node, goal_node)\n",
        "    if path:\n",
        "        print(\"A* path from\", start_node, \"to\", goal_node, \":\", path)\n",
        "        print(\"Total cost:\", cost)\n",
        "    else:\n",
        "        print(\"No path found from\", start_node, \"to\", goal_node)\n"
      ],
      "metadata": {
        "id": "rRHpN11-zD69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#expectimax\n",
        "import math\n",
        "\n",
        "def minimax(game_state, depth, maximizing_player, evaluation_function, alpha=float('-inf'), beta=float('inf')):\n",
        "    if depth == 0 or game_over(game_state):\n",
        "        return evaluation_function(game_state), None\n",
        "\n",
        "    if maximizing_player:\n",
        "        max_eval = float('-inf')\n",
        "        best_action = None\n",
        "        for action in legal_actions(game_state):\n",
        "            successor_state = generate_successor(game_state, action)\n",
        "            eval, _ = minimax(successor_state, depth - 1, False, evaluation_function, alpha, beta)\n",
        "            if eval > max_eval:\n",
        "                max_eval = eval\n",
        "                best_action = action\n",
        "            alpha = max(alpha, max_eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return max_eval, best_action\n",
        "    else:\n",
        "        min_eval = float('inf')\n",
        "        for action in legal_actions(game_state):\n",
        "            successor_state = generate_successor(game_state, action)\n",
        "            eval, _ = minimax(successor_state, depth - 1, True, evaluation_function, alpha, beta)\n",
        "            min_eval = min(min_eval, eval)\n",
        "            beta = min(beta, min_eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return min_eval, None\n",
        "\n",
        "def expectimax(game_state, depth, maximizing_player, evaluation_function):\n",
        "    if depth == 0 or game_over(game_state):\n",
        "        return evaluation_function(game_state), None\n",
        "\n",
        "    if maximizing_player:\n",
        "        max_eval = float('-inf')\n",
        "        best_action = None\n",
        "        for action in legal_actions(game_state):\n",
        "            successor_state = generate_successor(game_state, action)\n",
        "            eval, _ = expectimax(successor_state, depth - 1, False, evaluation_function)\n",
        "            if eval > max_eval:\n",
        "                max_eval = eval\n",
        "                best_action = action\n",
        "        return max_eval, best_action\n",
        "    else:\n",
        "        sum_eval = 0\n",
        "        num_actions = 0\n",
        "        for action in legal_actions(game_state):\n",
        "            successor_state = generate_successor(game_state, action)\n",
        "            eval, _ = expectimax(successor_state, depth - 1, True, evaluation_function)\n",
        "            sum_eval += eval\n",
        "            num_actions += 1\n",
        "        return sum_eval / num_actions, None\n",
        "\n",
        "# You'll need to implement the following functions specific to your game:\n",
        "# - legal_actions(game_state): Returns a list of legal actions from the given game state.\n",
        "# - generate_successor(game_state, action): Generates the successor game state after applying the given action.\n",
        "# - evaluation_function(game_state): Evaluates the given game state and returns a score.\n",
        "# - game_over(game_state): Checks if the game is over in the given game state.\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    initial_state = initialize_game()\n",
        "    depth_limit = 3\n",
        "    maximizer = True  # Whether the current player is the maximizer\n",
        "    score, action = minimax(initial_state, depth_limit, maximizer, evaluation_function)\n",
        "    print(\"Minimax score:\", score)\n",
        "    print(\"Best action:\", action)\n"
      ],
      "metadata": {
        "id": "AJSMKcmYyv7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}